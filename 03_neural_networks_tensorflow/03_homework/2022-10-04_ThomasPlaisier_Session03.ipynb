{"cells":[{"cell_type":"markdown","metadata":{},"source":[" # CIFAR-10 dataset classification with CNNs"]},{"cell_type":"markdown","metadata":{},"source":["# Homework: improve the accuracy of this model. Currently the model scores ~58% on the testing set.\n"," \n","I changed the follow settings:\n","Batch size: 512 --> 128\n","\n","Epochs: 3 --> 100\n","\n","Learning rate: 0.1 --> 0.001\n","\n","Optimizer: ADAM --> RMSprop\n","\n","Model: base CIFAR10Classifier --> CIFAR10ClassifierAug (more filters and dense layers).\n","\n","\n","End result:\n","\n","Validation accuracy 58% --> 72.8%.\n","From the graphs at the end of the notebook, training and testing accuracy stabilize after ~20 epochs. Training loss plateaus after about 20 epochs as well, while testing loss continually increases."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Imports.\n","import tensorflow as tf\n","\n","import numpy\n","import matplotlib.pyplot as plt\n","import time\n","from sklearn.metrics import confusion_matrix\n","import glob\n","import getopt\n","import sys\n","import datetime\n","\n","# Fixes issue with Tensorflow crashing?\n","import os\n","os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n","\n","# Image loader for importing from folder.\n","try:\n","    from image_dataset_loader import load\n","except:\n","    os.system(\"https_proxy=http://proxy.tmi.alcf.anl.gov:3128  pip install image-dataset-loader\")\n","    from image_dataset_loader import load\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found pre-shaped data. Loading from Numpy...\n","Data loaded.\n"]}],"source":["\n","# If the dataset has already been pre-processed, just load it from a stored Numpy array.\n","file = glob.glob('**/cifar_store.npz', recursive=True) # Search everywhere, because debugging launches the file from $HOME whereas qsub runs it from the homework folder.\n","if len(file) != 0:\n","    print(\"Found pre-shaped data. Loading from Numpy...\")\n","    this_file = numpy.load(file[0])\n","    x_train = this_file[\"x_train\"]\n","    x_test = this_file[\"x_test\"]\n","    y_train = this_file[\"y_train\"]\n","    y_test = this_file[\"y_test\"]\n","    print(\"Data loaded.\")\n","else:\n","    # No stored Numpy. Check if the cifar folder exists.\n","    folder = glob.glob('./cifar10')\n","    if len(folder) == 0:\n","        # Folder does not exist. Download from source.\n","\n","        # Fix to avoid invalid SSL certificates on Theta.\n","        import requests\n","        import ssl\n","        try:\n","            print(\"Downloading CIFAR10 with alternative SSL setup...\")\n","            requests.packages.urllib3.disable_warnings()\n","\n","            try:\n","                _create_unverified_https_context = ssl._create_unverified_context\n","            except AttributeError:\n","                # Legacy Python that doesn't verify HTTPS certificates by default.\n","                pass\n","            else:\n","                # Handle target environment that doesn't support HTTPS verification.\n","                ssl._create_default_https_context = _create_unverified_https_context\n","\n","            (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n","\n","        except:\n","            print(\"Downloading CIFAR10 with WGet...\")\n","            os.system(\"pip install wget\")\n","            import wget\n","            os.system(\"https_proxy=http://proxy.tmi.alcf.anl.gov:3128  wget https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz\")\n","            os.system(\"tar -xf cifar10.tgz\")\n","    else:\n","        print(\"Folder found. Loading with image_dataset_loader...\")\n","        # Use image-dataset-loader to import the data. If this doesn't work, restart the kernel to refresh the package. If it does work, it takes _ages_.\n","        (x_train, y_train), (x_test, y_test) = load('cifar10', ['train', 'test'])\n","        print(\"Data loaded.\")\n","    \n","    # After loading raw data from folder, shape.\n","    x_train = x_train.astype(numpy.float32)\n","    x_test  = x_test.astype(numpy.float32)\n","\n","    x_train /= 255.\n","    x_test  /= 255.\n","\n","    y_train = y_train.astype(numpy.int32)\n","    y_test  = y_test.astype(numpy.int32)\n","    # Save shaped data to file.\n","    with open(\"cifar_store.npz\", \"wb\") as f:\n","        numpy.savez(f, x_train=x_train, x_test=x_test, y_train=y_train, y_test=y_test)\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Definitions\n","\n","# CIFAR10 Classifier class.\n","class CIFAR10Classifier(tf.keras.models.Model):\n","\n","    def __init__(self, activation=tf.nn.tanh):\n","        tf.keras.models.Model.__init__(self)\n","\n","        # Filter layer: 32 3x3 kernels.\n","        self.conv_1 = tf.keras.layers.Conv2D(32, [3, 3], activation='relu')\n","        # Filter layer: 64 3x3 kernels.\n","        self.conv_2 = tf.keras.layers.Conv2D(64, [3, 3], activation='relu')\n","        # Subsampling layer: max pooling in 2x2.\n","        self.pool_3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))\n","        # Regularization.\n","        self.drop_4 = tf.keras.layers.Dropout(0.25)\n","        # Dense 128 output layer.\n","        self.dense_5 = tf.keras.layers.Dense(128, activation='relu')\n","        # Regularization.\n","        self.drop_6 = tf.keras.layers.Dropout(0.5)\n","        # Dense 10 class output layer.\n","        self.dense_7 = tf.keras.layers.Dense(10, activation='softmax')\n","\n","    def call(self, inputs):\n","\n","        x = self.conv_1(inputs)\n","        x = self.conv_2(x)\n","        x = self.pool_3(x)\n","        x = self.drop_4(x)\n","        # Flatten because we will be feeding to dense layers next.\n","        x = tf.keras.layers.Flatten()(x)\n","        x = self.dense_5(x)\n","        x = self.drop_6(x)\n","        x = self.dense_7(x)\n","\n","        return x\n","\n","# CIFAR classifier with more filter layers, more dense layers, and no dropout.\n","class CIFAR10ClassifierAug(tf.keras.models.Model):\n","\n","    def __init__(self, activation=tf.nn.tanh):\n","        tf.keras.models.Model.__init__(self)\n","\n","        # Filter layer: 32 3x3 kernels.\n","        self.conv_1 = tf.keras.layers.Conv2D(32, [3, 3], padding=\"same\", activation='relu')\n","        # Filter layer: 32 3x3 kernels.\n","        self.conv_2 = tf.keras.layers.Conv2D(32, [3, 3], padding=\"same\", activation='relu')\n","        # Subsampling layer: max pooling in 2x2.\n","        self.pool_1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))\n","        # Regularization layer: 25% dropout.\n","        self.drop_1 = tf.keras.layers.Dropout(0.25)\n","        # Filter layer: 64 3x3 kernels.\n","        self.conv_3 = tf.keras.layers.Conv2D(64, [3, 3], padding=\"same\", activation='relu')\n","        # Filter layer: 64 3x3 kernels.\n","        self.conv_4 = tf.keras.layers.Conv2D(64, [3, 3], padding=\"same\", activation='relu')\n","        # Subsampling layer: max pooling in 2x2.\n","        self.pool_2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))\n","        # Regularization layer: 25% dropout.\n","        self.drop_2 = tf.keras.layers.Dropout(0.25)\n","        # Filter layer: 128 3x3 kernels.\n","        self.conv_5 = tf.keras.layers.Conv2D(128, [3, 3], padding=\"same\", activation='relu')\n","        # Filter layer: 128 3x3 kernels.\n","        self.conv_6 = tf.keras.layers.Conv2D(128, [3, 3], padding=\"same\", activation='relu')\n","        # Subsampling layer: max pooling in 2x2.\n","        self.pool_3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))\n","        # Regularization layer: 25% dropout.\n","        self.drop_3 = tf.keras.layers.Dropout(0.25)\n","        # Dense 512 output layer.\n","        self.dense_1 = tf.keras.layers.Dense(512, activation='relu')\n","        # Dense 128 output layer.\n","        self.dense_2 = tf.keras.layers.Dense(128, activation='relu')\n","        # Dense 32 output layer.\n","        self.dense_3 = tf.keras.layers.Dense(32, activation='relu')\n","        # Dense 10 output layer.\n","        self.dense_4 = tf.keras.layers.Dense(10, activation='relu')\n","        # Regularization layer: 25% dropout.\n","        self.drop_4 = tf.keras.layers.Dropout(0.25)\n","        # Dense 10 class output layer.\n","        self.dense_class = tf.keras.layers.Dense(10, activation='softmax')\n","\n","    def call(self, inputs):\n","\n","        x = self.conv_1(inputs)\n","        x = self.conv_2(x)\n","        x = self.pool_1(x)\n","        x = self.drop_1(x)\n","        x = self.conv_3(x)\n","        x = self.conv_4(x)\n","        x = self.pool_2(x)\n","        x = self.drop_2(x)\n","        x = self.conv_5(x)\n","        x = self.conv_6(x)\n","        x = self.pool_3(x)\n","        x = self.drop_3(x)\n","        # Flatten because we will be feeding to dense layers next.\n","        x = tf.keras.layers.Flatten()(x)\n","        x = self.dense_1(x)\n","        x = self.dense_2(x)\n","        x = self.dense_3(x)\n","        x = self.dense_4(x)\n","        x = self.drop_4(x)\n","        x = self.dense_class(x)\n","\n","        return x\n","\n","# Loss function of model.\n","def compute_loss(y_true, y_pred):\n","    # If labels were one-hot encoded, use standard crossentropy.\n","    # Since labels are integers, use sparse categorical cross-entropy. \n","    # The network's final layer is softmax, so from_logtis=False\n","    scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n","    \n","    return scce(y_true, y_pred)  \n","\n","# Forward pass of model.\n","def forward_pass(model, batch_data, y_true):\n","    y_pred = model(batch_data)\n","    loss = compute_loss(y_true, y_pred)\n","\n","    return loss\n","\n","# Training loop manager.\n","def train_loop(dataset_train, dataset_test, batch_size, n_training_epochs, model, optimizer, silent = False):\n","    \n","    @tf.function()\n","    def train_iteration(batch_data, y_true, model, optimizer):\n","        # GradientTape keeps track of the gradients as they are calculated in the iterations. This lets you define a custom training loop.\n","        with tf.GradientTape() as tape:\n","            loss = forward_pass(model, batch_data, y_true)\n","\n","        # In the Keras.io examples, they use trainable_weights?\n","        trainable_vars = model.trainable_variables\n","\n","        # Apply the update to the network (one at a time):\n","        grads = tape.gradient(loss, trainable_vars)\n","\n","        # Keras.io: note that this does not apply gradient clipping: you'd have to do that manually.\n","        optimizer.apply_gradients(zip(grads, trainable_vars))\n","        return loss\n","\n","    def validation_iteration(batch_data, y_true, model, optimizer):\n","        with tf.GradientTape() as tape:\n","            loss = forward_pass(model, batch_data, y_true)\n","        return loss\n","\n","    # Initialize.\n","    avg_time = 0\n","    total_time = 0\n","    loss_train = numpy.zeros([n_training_epochs, 1])\n","    loss_test = numpy.zeros([n_training_epochs, 1])\n","    acc_train = numpy.zeros([n_training_epochs, 1])\n","    acc_test = numpy.zeros([n_training_epochs, 1])\n","\n","    for i_epoch in range(n_training_epochs):\n","        start = time.time()\n","        if not silent:\n","            print(\"Epoch %d\" % i_epoch)\n","        \n","        # Shuffle the whole dataset.\n","        dataset_train.shuffle(50000) \n","        # Create a list of batches to iterate through.\n","        batches = dataset_train.batch(batch_size=batch_size, drop_remainder=True)\n","        loss_batch = numpy.zeros([len(batches), 1])\n","        acc_batch = numpy.zeros([len(batches), 1])\n","        for i_batch, (batch_data, y_true) in enumerate(batches):\n","            batch_data = tf.reshape(batch_data, [-1, 32, 32, 3])\n","            loss_batch[i_batch] = train_iteration(batch_data, y_true, model, optimizer)\n","            acc_batch[i_batch] = get_accuracy(model, batch_data, y_true, 10)[0]\n","        \n","        # Average loss across all batches.\n","        loss_train[i_epoch] = numpy.mean(loss_batch)\n","        # Get classification accuracy of training set.\n","        acc_train[i_epoch] = numpy.mean(acc_batch)\n","\n","        # Shuffle the whole dataset.\n","        dataset_test.shuffle(10000) \n","        # Create a list of batches to iterate through.\n","        batches = dataset_test.batch(batch_size=10000, drop_remainder=True)\n","        loss_batch = numpy.zeros([len(batches), 1])\n","        acc_batch = numpy.zeros([len(batches), 1])\n","        for i_batch, (batch_data, y_true) in enumerate(batches):\n","            batch_data = tf.reshape(batch_data, [-1, 32, 32, 3])\n","            loss_batch[i_batch] = validation_iteration(batch_data, y_true, model, optimizer)\n","            acc_batch[i_batch] = get_accuracy(model, batch_data, y_true, 10)[0]\n","      \n","        # Average loss across all batches.\n","        loss_test[i_epoch] = numpy.mean(loss_batch)\n","        # Get classification accuracy of training set.\n","        acc_test[i_epoch] = numpy.mean(acc_batch)\n","\n","        end = time.time()\n","        total_time += (end-start)\n","        avg_time = total_time / (i_epoch + 1)\n","        if not silent:\n","            print(\"Took %1.1f seconds for epoch #%d.\" % (end-start, i_epoch))\n","\n","    print(\"Took %.1f s in total. (avg: %.3f / epoch)\" % (total_time, avg_time))\n","\n","    return acc_train, acc_test, loss_train, loss_test\n","\n","# Training function.\n","def train_network(dataset_train, dataset_test, _model_type, _optimizer, _batch_size, _n_training_epochs, _lr, _silent = False):\n","\n","    # Instantiate model.\n","    if _model_type == \"base\":\n","        mnist_model = CIFAR10Classifier()\n","    elif _model_type == \"aug\":\n","        mnist_model = CIFAR10ClassifierAug()\n","    else:\n","        mnist_model = CIFAR10Classifier()\n","\n","    # Define optimizer.\n","    if _optimizer == \"adam\":\n","        optimizer = tf.keras.optimizers.Adam(learning_rate=_lr)\n","    if _optimizer == \"rmsprop\":\n","        optimizer = tf.keras.optimizers.RMSprop(learning_rate=_lr)\n","    elif _optimizer == \"sgd\":\n","        mnist_model = tf.keras.optimizers.SGD(learning_rate=_lr)\n","    else:\n","        optimizer = tf.keras.optimizers.Adam(_lr)\n","\n","    # Train model with given hyperparameters.\n","    acc_train, acc_test, loss_train, loss_test = train_loop(dataset_train, dataset_test, _batch_size, _n_training_epochs, mnist_model, optimizer, _silent)\n","\n","    return mnist_model, acc_train, acc_test, loss_train, loss_test\n","\n","def get_accuracy(model, batch_data, batch_labels, n_classes):\n","    predictions = model.predict(batch_data)\n","    cm = confusion_matrix(batch_labels, numpy.argmax(predictions, axis=1), labels=list(range(n_classes)))\n","\n","    j_sum = 0\n","    for i,j in enumerate(cm.diagonal()/cm.sum(axis=1)): \n","        j_sum += j\n","    acc = 100*j_sum/n_classes\n","    return acc, cm"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Invalid input. Running from notebook?\n"]}],"source":["# Argument parser\n","\n","# Default values.\n","batch_size = 128\n","epochs = 20\n","learning_rate = 0.001\n","model_type = \"aug\"\n","optimizer_type = \"rmsprop\"\n","silent = True\n","arg_help = \"{0} -m <model_type> -o <optimizer_type> -b <batch_size> -e <epochs> -l <learning_rate> -s <silent>\".format(sys.argv[0])\n","# Validate input.\n","try:\n","    opts, args = getopt.getopt(sys.argv[1:], \"hm:o:b:e:l:s:\", [\"help\", \"model_type=\", \"optimizer_type=\" \"batch_size=\", \n","    \"epochs=\", \"learning_rate=\", \"silent=\"])\n","except:\n","    print(\"Invalid input. Running from notebook?\")\n","    # print(sys.argv)\n","    # print(arg_help)\n","    opts = []\n","\n","# Assign arguments.\n","for opt, arg in opts:\n","    if opt in (\"-h\", \"--help\"):\n","        print(arg_help)\n","        sys.exit()\n","    elif opt in (\"-m\", \"--model_type\"):\n","        model_type = arg.lower()\n","    elif opt in (\"-o\", \"--optimizer_type\"):\n","        optimizer_type = arg.lower()\n","    elif opt in (\"-b\", \"--batch_size\"):\n","        batch_size = int(arg)\n","    elif opt in (\"-e\", \"--epochs\"):\n","        epochs = int(arg)\n","    elif opt in (\"-l\", \"--learning_rate\"):\n","        learning_rate = float(arg)\n","    elif opt in (\"-s\", \"--silent\"):\n","        silent = arg.lower() == 'true'\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Create datasets.\n","dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","dataset_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train model.\n","print(\"Training model with hyperparameters:\")\n","print(\"Model: %s. Optimizer: %s. BS: %i. Epochs: %i. LR: %f.\" % (model_type, \n","optimizer_type, batch_size, epochs, learning_rate))\n","\n","model, acc_train, acc_test, loss_train, loss_test = train_network(dataset_train, dataset_test, model_type, optimizer_type, batch_size, epochs, learning_rate, silent)\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["313/313 [==============================] - 6s 19ms/step\n","\n","Confusion matrix (rows: true classes; columns: predicted classes):\n","[[804   3  69  14   5   1   3   5  72  24]\n"," [ 20 819   7   4   3   1  12   6  30  98]\n"," [ 57   1 737  52  56  21  40  17  13   6]\n"," [ 33   3 116 578  50  70  89  19  18  24]\n"," [ 36   3 109  91 590  24  30 102   6   9]\n"," [ 28   2 121 244  46 467  35  38   8  11]\n"," [  6   3  65  60  31  18 783   7  19   8]\n"," [ 31   3  63  51  33  33   3 756   5  22]\n"," [ 55  14  10  11   2   3   5   0 888  12]\n"," [ 40  35   9   2   5   4   6   4  33 862]]\n","\n","Model: aug. Optimizer: rmsprop. Acc: 72.8. BS: 128. Epochs: 20. LR: 0.001000.\n"]}],"source":["# Print confusion matrix and accuracy for the testing data.\n","acc, cm = get_accuracy(model, x_test, y_test, 10)\n","print()\n","print('Confusion matrix (rows: true classes; columns: predicted classes):')\n","print(cm)\n","print()\n","\n","print(\"Model: %s. Optimizer: %s. Acc: %.1f. BS: %i. Epochs: %i. LR: %f.\" % (model_type, optimizer_type, acc, batch_size, epochs, learning_rate))\n","\n","# Get datestring of now.\n","dt_string = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# %%\n","# Create accuracy and loss figures.\n","# plot loss\n","plt.tight_layout()\n","plt.subplot(211)\n","plt.plot(loss_train, color='blue', label='train')\n","plt.plot(loss_test, color='red', label='test')\n","plt.title('Loss')\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss Value\")\n","plt.legend([\"Training\", \"Testing\"])\n","# plot accuracy\n","plt.subplot(212)\n","plt.plot(acc_train, color='blue', label='train')\n","plt.plot(acc_test, color='red', label='test')\n","plt.title('Classification Accuracy')\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy %\")\n","plt.legend([\"Training\", \"Testing\"])\n","# save plot to file\n","filename = sys.argv[0].split('/')[-1]\n","plt.savefig(dt_string + '_plot.png')\n","plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save model.\n","file_name = \"%s_model_%s_opt_%s_acc_%.1f_BS_%i_LR_%.4f.tf\" % (\n","    dt_string, model_type, optimizer_type, acc, batch_size, learning_rate)\n","model.save(file_name)\n","print(\"Model saved to '%s'.\" % (file_name))\n"]},{"cell_type":"markdown","metadata":{},"source":["History plot:\n","\n","<img src=\"2022_10_18_12_23_08_plot.png\" alt=\"Loss and accuracy history\">"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"ac3fec1943ff5b9ab25a41a15ea8714bc35b39a178b31cfcaac1c0eb593ed0fe"}}},"nbformat":4,"nbformat_minor":2}
